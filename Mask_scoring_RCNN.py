# -*- coding: utf-8 -*-
"""DL_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14xZhfFXmQX5L8nsJZJ0NEkEj_ulm_HOk
"""

!nvcc -V
!gcc --version

# Commented out IPython magic to ensure Python compatibility.
# #create virtual environment
!conda create -n openmmlab python=3.7 -y
!source activate openmmlab

 # install dependencies
!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html

# # install mmcv-full thus we could use CUDA operators
!pip install mmcv-full==1.3.17 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html 

!pip install openmim --use-feature=2020-resolver
!mim install mmdet

# Install mmdetection
!rm -rf mmdetection
!git clone https://github.com/open-mmlab/mmdetection.git
# %cd mmdetection

!pip install -r requirements/build.txt 
!pip install -v -e .

!pip install imagehash

#import Lib
import torch, torchvision
import glob
import os
import numpy as np
import cv2
import random
import itertools
import pandas as pd
#from tqdm import tqdm
import urllib
import json
import PIL.Image as Image
import shutil
from os.path import exists
from tqdm.auto import  tqdm
import imagehash
import matplotlib.pyplot as plt
import random

!unzip '/content/archive.zip' -d '/content/data'

annotationsPath = '/content/data/annotations'
imagesPath = '/content/data/images'

img = cv2.imread('/content/data/images/maksssksksss160.png')

img = cv2.cvtColor(img,  cv2.COLOR_RGB2BGR)

plt.figure(figsize=(12,12))
plt.imshow(img)

xmlfiles = []
imgfiles = []

xmlfiles = [ f for f in os.listdir(annotationsPath) if os.path.isfile(os.path.join(annotationsPath,f)) ]
xmlfiles = sorted(xmlfiles)

imgfiles = [ f for f in os.listdir(imagesPath ) if os.path.isfile(os.path.join(imagesPath ,f)) ]
imgfiles = sorted(imgfiles)

print(xmlfiles[0:3])
print(imgfiles[0:3])

assert len(xmlfiles) == len(imgfiles)

temp = list(zip(xmlfiles, imgfiles))
random.shuffle(temp)
xmlfiles, imgfiles = zip(*temp)

print(xmlfiles[0:3])
print(imgfiles[0:3])

train_img, test_img = np.split(np.array(imgfiles), [int(len(imgfiles)* (1 - 0.25))])
print(train_img[0])
print(test_img[0:3])

train_xml, test_xml = np.split(np.array(xmlfiles), [int(len(xmlfiles)* (1 - 0.25))])
print(train_xml[0])
print(test_xml[0])

train_imgs = []
test_imgs = []

train_imgs = [imagesPath + '/' + name for name in train_img]
test_imgs = [imagesPath + '/' + name for name in test_img]
print(train_imgs[0])
print(test_imgs[0])

train_xmls = []
test_xmls = []

train_xmls = [annotationsPath+ '/' + name for name in train_xml.tolist()]
test_xmls = [annotationsPath + '/' + name for name in test_xml.tolist()]
print(train_xmls[0])
print(test_xmls[0])

!mkdir -p '/content/train/images'
!mkdir -p '/content/test/images'
!mkdir -p '/content/train/annotations'
!mkdir -p '/content/test/annotations'

for name in train_imgs:
  shutil.copy(name, '/content/train/images')

for name in test_imgs:
  shutil.copy(name, '/content/test/images')

for name in train_xmls:
  shutil.copy(name, '/content/train/annotations')

for name in test_xmls:
  shutil.copy(name, '/content/test/annotations')

with open('./train_xml_list.txt', 'w') as f:
    for item in train_xml:
        f.write("%s\n" % item)

with open('./test_xml_list.txt', 'w') as f:
    for item in test_xml:
        f.write("%s\n" % item)

"""Convert to COCO add mask information"""

!pip install lxml

import sys
import os
import json
import xml.etree.ElementTree as ET


START_BOUNDING_BOX_ID = 1
PRE_DEFINE_CATEGORIES = {}
# If necessary, pre-define category and its id
#  PRE_DEFINE_CATEGORIES = {"aeroplane": 1, "bicycle": 2, "bird": 3, "boat": 4,
                         #  "bottle":5, "bus": 6, "car": 7, "cat": 8, "chair": 9,
                         #  "cow": 10, "diningtable": 11, "dog": 12, "horse": 13,
                         #  "motorbike": 14, "person": 15, "pottedplant": 16,
                         #  "sheep": 17, "sofa": 18, "train": 19, "tvmonitor": 20}

def set_mask():
  return



def get(root, name):
    vars = root.findall(name)
    return vars


def get_and_check(root, name, length):
    vars = root.findall(name)
    if len(vars) == 0:
        raise NotImplementedError('Can not find %s in %s.'%(name, root.tag))
    if length > 0 and len(vars) != length:
        raise NotImplementedError('The size of %s is supposed to be %d, but is %d.'%(name, length, len(vars)))
    if length == 1:
        vars = vars[0]
    return vars


def get_filename_as_int(filename):
    try:
        filename = os.path.splitext(filename)[0]
        #filename = filename.strip("maksssksksss")
        #print(filename)
        return filename
    except:
        raise NotImplementedError('Filename %s is supposed to be an integer.'%(filename))


def convert(xml_list, xml_dir, json_file):
    list_fp = open(xml_list, 'r')
    json_dict = {"images":[], "type": "instances", "annotations": [],
                 "categories": []}
    categories = PRE_DEFINE_CATEGORIES
    bnd_id = START_BOUNDING_BOX_ID
    for line in list_fp:
        line = line.strip()
        #print("Processing %s"%(line))
        xml_f = os.path.join(xml_dir, line)
        tree = ET.parse(xml_f)
        root = tree.getroot()
        path = get(root, 'path')

        if len(path) == 1:
            filename = os.path.basename(path[0].text)
        elif len(path) == 0:
            filename = get_and_check(root, 'filename', 1).text
        else:
            raise NotImplementedError('%d paths found in %s'%(len(path), line))

        ## The filename must be a number
        #print("filename is %s" %(filename))
        image_id = get_filename_as_int(filename)
        size = get_and_check(root, 'size', 1)
        width = int(get_and_check(size, 'width', 1).text)
        height = int(get_and_check(size, 'height', 1).text)
        image = {'file_name': filename, 'height': height, 'width': width,
                 'id':image_id}
        json_dict['images'].append(image)




        for obj in get(root, 'object'):
            category = get_and_check(obj, 'name', 1).text
            if category not in categories:
                new_id = len(categories)
                categories[category] = new_id
            category_id = categories[category]
            bndbox = get_and_check(obj, 'bndbox', 1)
            xmin = int(get_and_check(bndbox, 'xmin', 1).text) - 1
            ymin = int(get_and_check(bndbox, 'ymin', 1).text) - 1
            xmax = int(get_and_check(bndbox, 'xmax', 1).text)
            ymax = int(get_and_check(bndbox, 'ymax', 1).text)
            mask = [xmin, ymin, xmax, ymin, xmax, ymax, xmin, ymax]# mask annotations based on the bbox 



            assert(xmax > xmin)
            assert(ymax > ymin)
            o_width = abs(xmax - xmin)
            o_height = abs(ymax - ymin)
            ann = {'segmentation': [mask], 'area': o_width*o_height, 'iscrowd': 0, 'image_id':#add segmentation/mask polygon format
                   image_id, 'bbox':[xmin, ymin, o_width, o_height],
                   'category_id': category_id, 'id': bnd_id, 'ignore': 0,
                   }

            json_dict['annotations'].append(ann)
            bnd_id = bnd_id + 1




    for cate, cid in categories.items():
        cat = {'supercategory': 'none', 'id': cid, 'name': cate}
        json_dict['categories'].append(cat)

    json_fp = open(json_file, 'w')
    json_str = json.dumps(json_dict)
    json_fp.write(json_str)
    json_fp.close()
    list_fp.close()

open('/content/train/train_annotations.json', 'w')
open('/content/test/test_annotations.json', 'w')

convert('train_xml_list.txt', '/content/train/annotations/', '/content/train/train_annotations.json')
convert('test_xml_list.txt', '/content/test/annotations', '/content/test/test_annotations.json')

from mmcv import collect_env
collect_env()

# Check MMDetection installation
import mmdet
print(mmdet.__version__)

# Check mmcv installation
from mmcv.ops import get_compiling_cuda_version, get_compiler_version
print(get_compiling_cuda_version())
print(get_compiler_version())

#parse the order of categories in the json file
import json

with open('/content/train/train_annotations.json', 'r') as json_file:
    json_load = json.load(json_file)

CLASSES = []
for i in range(3):
    #print(json_load['categories'][i]['name'])
    CLASSES.append(json_load['categories'][i]['name'])

print(CLASSES)

if not exists('./workdirs'):
    os.makedirs('./workdirs')

from mmcv import Config

def get_device():
  is_device_available = {
  'cuda': torch.cuda.is_available(),
  }
  device_list = [k for k, v in is_device_available.items() if v]
  return device_list[0] if len(device_list) == 1 else 'cpu'




#cfg = Config.fromfile('./configs/empirical_attention/faster_rcnn_r50_fpn_attention_0010_dcn_1x_coco.py')
cfg = Config.fromfile('./configs/ms_rcnn/ms_rcnn_r101_caffe_fpn_2x_coco.py')

from mmdet.apis import set_random_seed
cfg["device"] = get_device()
# Modify dataset type and path
cfg.data_root = '/content/'

cfg.data.test.classes = (CLASSES[0], CLASSES[1], CLASSES[2])
cfg.data.test.ann_file = cfg.data_root + 'train/train_annotations.json'
cfg.data.test.img_prefix = cfg.data_root + 'train/images/'

cfg.data.val.classes = (CLASSES[0], CLASSES[1], CLASSES[2])
cfg.data.val.ann_file = cfg.data_root + 'test/test_annotations.json'
cfg.data.val.img_prefix = cfg.data_root + 'test/images/'

## modify the training process
cfg.img_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
cfg.train_pipeline = [
        dict(type='LoadImageFromFile'),
        dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
        dict(
            type='Resize',
            img_scale=[(1333, 640), (1333, 800)],
            multiscale_mode='range',
            keep_ratio=True),
        dict(type='RandomFlip', flip_ratio=0.5),
        dict(type='Normalize', **cfg.img_norm_cfg),
        dict(type='Pad', size_divisor=32),
        dict(type='DefaultFormatBundle'),
        dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels','gt_masks']),]#get mask annotations

cfg.data.train = dict(
            type='RepeatDataset',
            times=3,
            dataset=dict(
                type='CocoDataset',
                classes = (CLASSES[0], CLASSES[1], CLASSES[2]),
                ann_file = cfg.data_root + 'train/train_annotations.json',
                img_prefix = cfg.data_root + 'train/images/',
                pipeline = cfg.train_pipeline))

# modify num classes 
cfg.model.roi_head.bbox_head.num_classes = 3
cfg.model.roi_head.mask_head.num_classes = 3
cfg.model.roi_head.mask_iou_head.num_classes = 3



#set the working directory
cfg.work_dir = './workdirs'

# LR for one GPU
cfg.optimizer.lr = 0.02 / 8

# Set seed thus the results are more reproducible
cfg.seed = 0
set_random_seed(0, deterministic=False)
cfg.gpu_ids = range(1)

# change it to whatever you like
cfg.runner.max_epochs = 12

# set different configs for every data
cfg_train = cfg
cfg_test = cfg
cfg_inference = cfg
cfg_video = cfg

# We can initialize the logger for training and have a look
# at the final config used for training
print(f'Config:\n{cfg.pretty_text}')

import mmcv
import os.path as osp
from mmdet.datasets import build_dataset
from mmdet.models import build_detector
from mmdet.apis import train_detector


# Build dataset
datasets = [build_dataset(cfg_train.data.train)]

# Build the detector
model = build_detector(cfg_train.model)

# Add an attribute for visualization convenience
model.CLASSES = datasets[0].CLASSES

# Create work_dir
mmcv.mkdir_or_exist(osp.abspath(cfg_train.work_dir))
train_detector(model, datasets, cfg_train, distributed=False, validate=True)

!python tools/analysis_tools/analyze_logs.py plot_curve /content/None.log.json --keys loss_mask --legend loss_mask --out /content/losses_mask.pdf

#!python tools/analysis_tools/analyze_logs.py plot_curve /content/None.log.json --keys loss_cls loss_bbox --out /content/losses.pdf

!python tools/analysis_tools/analyze_logs.py cal_train_time /content/None.log.json